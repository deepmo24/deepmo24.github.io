---
title: 循环神经网络(RNN)学习笔记
author: Langyuan Mo
mathjax: true
categories: 学习笔记
tags:
    - 神经网络
    - 深度学习
---


## 前言
这是我的学习笔记，记录自己的学习过程。一来可以巩固自己对知识的了解，二来也方便日后温故而知新。同时也欢迎大家阅览交流。

## RNN的背景知识
RNN(Recurrent Neuroal Network),又叫循环神经网络，最早在1990年左右提出，一开始由于硬件条件不足和不能解决长依赖问题，RNN并没有流行。后来RNN不断发展，其中具有代表性的两个变体是lstm(1997)和gru(2014)，这两个今天在实践中也是常用的。

学习RNN之前需要了解传统的神经网络，我入门的时候看的是[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)这本电子书。

## RNN的详细介绍
<!-- more -->

### RNN用来做什么
RNN擅长处理序列信息(sequence)，所以可以用RNN来对序列信息建模。如时间序列信息，可以用RNN来预测回归问题；如句子序列信息，可以用RNN来处理NLP的问题。

### 什么是RNN
RNN的动机(motivation)是利用序列信息。在传统的神经网络中我们假设所有的输入（和输出）都是互相独立的，但是这对于很多任务并不是一个好的想法。如果你想在一个句子中预测下一个词，你最好能够知道预测的词前面的这些词。RNN之所以叫循环神经网络，就是因为它对序列中的每一个元素都执行相同的工作，RNN的输出依赖于之前的计算。我们也可以这样理解RNN，它有一个“记忆”单元，能够记住之前计算的信息，然后利用现在的信息和之前的信息来预测下一个信息。

需要注意的是理论上RNN能够记住之前任意长的序列信息，但是在实际中它被限制为记住一定步长的序列信息。

典型的RNN如下图所示：
![loading](/images/rnn1.jpg)
> 对RNN的理解要区别于传统神经网络，上面的RNN展开图可以想象成多个传统神经网络的复制互连(隐藏层互连)，它们共享参数。下图可以有助理解：

![loading](/images/rnn_open.jpg)


### RNN的原理
理解RNN的原理就从理解RNN的每个变量以及变量之间的计算入手。
仍然看RNN的展开图，每个变量的意义如下：
$x_t$: step t的输入。

$s_t$: step t的隐藏单元，“记忆”单元。

$o_t$: step t的输出。

* RNN的三个参数（U,W,V）在每一层中都是相同的，即每一层共享这三个参数。这与传统神经网络不同。
* RNN每层都有输出，输出有时并不是那么重要，在一些任务中只关注最后的一个输出，甚至不关注输出。RNN的主要特征是隐藏层，因为隐藏层有序列的“记忆”。

RNN层之间的计算：
$$s_t=tanh(Ux_t+Ws_{t-1})$$
$$o_t=softmax(Vs_t)$$

可以看出，当前的隐藏层与之前的隐藏层相关。tanh()是激活函数。softmax()将输出归一化。其实上面的式子也可以在tanh()和softmax()里面的式子加上一个偏差bias。

下面我们来量化理解下，假设隐藏层单元维数H=100,输入的维度为C=8000，那么：

\begin{align*}
&x_t \in R^{8000} \\ 
&s_t \in R^{100}  \\
&o_t \in R^{8000} \\
&U \in R^{100 \times 8000} \\
&V \in R^{8000 \times 100} \\
&W \in R^{100 \times 100}
\end{align*}

参数的计算量为：$2HC+H^2$。可以看到计算的瓶颈在C的维度，因为H的维度我们可以手动设置，一般不会太高。

### RNN的训练
RNN的训练需要确定损失函数和训练方法。分类问题损失函数一般选取交叉熵函数，回归问题一般选择均方差。训练方法为梯度下降法。**重点理解RNN如何求梯度。**

RNN采用bptt(Backpropagation Through Time)算法求梯度。bptt只是bp的一个扩展，跟bp不同之处在于其中一些参数依赖前面的状态，而不仅仅是当前状态。这在参数求导时会利用链式法则，对t step前的每一步的导数加起来表示t步的参数的导数。具体推导看[数学 · RNN（二）· BPTT 算法](https://zhuanlan.zhihu.com/p/26892413)。

**对bp算法的理解曾经踩过的坑：神经网络的参数求导 $\neq$ 损失函数的参数求导**
>在传统机器学习中，对损失函数的参数求导可以直接计算。但是在神经网络中，损失函数只是在最后一层计算，对它的参数求导也只是涉及最后一层网络的参数，前面的参数求导不能通过直接计算得到（需通过链式法则求导）。bp算法就是提供了一个求所有层参数的导数的桥梁。

### RNN的弊端——梯度消失问题
因为bptt对其中一些参数求导利用链式法则，当往前依赖越多，求导链越长，而问题就出在这里。tanh或sigmoid激活函数的导数的值在\[0,1\](sigmoid在\[0,0.25\])之间，在所有层之中，一些层的神经元激活函数值趋于0或1（饱和状态）,梯度到这里就趋于0,这将导致前面层的参数的梯度也趋于0，所以造成梯度消失。远离t(当前步)的层的梯度贡献为0，这解释了RNN不能处理长依赖。

**梯度消失问题不仅RNN有，这是所有深度前馈网络都会碰到的问题。只是RNN一般都很深（在语言模型中是句子的长度）,在RNN中这个问题特别明显。**

### 解决梯度消失问题:
合理的初始化参数、正则、通过使用ReLU激活函数代替tanh或sigmoid函数、或者使用RNN的变体lSTM或GRU(LSTM的简化版)等等。

## RNN的变体
* 双向RNN
* 深度双向RNN
* LSTM
* GRU(LSTM的变体)


## LSTM
**LSTM绕开了普通RNN的梯度消失问题，是最为流行的RNN类网络之一。**

在RNN中，我们计算隐藏层状态$s_t=tanh(Ux_t+Ws_{t-1})$。LSTM单元做的正是同样的事情。**这是从宏观上理解LSTM的关键。**

LSTM把原来的tanh结构改造成另一个结构。下面给两个对比图：
![loading](/images/LSTM3-SimpleRNN.png)
>普通的RNN关于隐藏层的结构

![loading](/images/LSTM3-chain.png)
>LSTM关于隐藏层的结构

LSTM的新结构有几个重点：
* LSTM有三个门，forget门，input门，output门。通过这些“门”，LSTM摒弃没用的信息，保留有用的信息,产生新的信息。
* LSTM多了一个记忆单元:$C_t$,但是这与隐藏层并不矛盾，记忆单元Ｃ是LSTM内部结构计算用的，隐藏层h除此之外还可以作为特征被外部使用（其实C可不可以给外部使用我也不清楚）。
* LSTM的这些结构使得它没有了梯度消失问题，可以保留任意长的序列信息。
* LSTM的结构用几条等式就可以表示。

LSTM结构的具体等式和详细推导看[这里](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

我更喜欢的一个LSTM的理解，结合Tensorflow源码理解的[博客](http://blog.gdf.name/lstm-with-tensorflow/)。

<!-- ## GRU
只有两个“门”，一个reset门，一个update门。跟lstm比没有了内部存储器c，结构比lstm简单一点。
公式化表示（待补充）


## LSTM与GRU的比较
LSTM与GRU都可以解决梯度消失的问题（如何绕过？待理解），你可能会想知道：使用哪一个？ GRU是相当新的（2014年），其权衡还没有得到充分的探索。 在许多任务中，两种体系结构都可以产生可比较的性能，而调整超参数（如层大小）可能比选择理想的体系结构更重要。 **GRU具有较少的参数（U和W较小），因此可能训练速度更快或需要更少的数据来推广。 另一方面，如果你有足够的数据，LSTM的更强大的表现力可能会带来更好的结果。** -->

## 总结
RNN并没有那么神秘，它也是我们触手可及的。不要觉得它们高大上而不敢去触碰它们，走近一点，你会发现RNN也只是几条公式而已嘛。共勉！






