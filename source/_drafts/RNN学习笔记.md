---
title: RNN学习笔记
tags:
---

## 什么是RNN
RNN的提出是想利用序列信息，在传统的神经网络中所有的输入都是互相独立的，然而在很多情况下我们需要知道一些过去信息（如在一个句子中预测下一个词）。

[这里插入一个RNN图片]

**重点理解各个输入，中间状态量，输出**

$x_t$: 在step t的输入，是一个n*1的向量

$s_t$: 在step t的隐藏单元，“记忆”功能，$s_t=f(Ux+W_{s_{t-1}}$

$o_t$: 在step t的输出，是一个表示所有单词归一化后的概率的向量，维数是V*1,V是字典

**RNN更新梯度采用BPTT，更新t的梯度需要用到t前面所有的信息**
<!-- more -->

## RNN的变体
* 双向RNN
* 深度双向RNN
* LSTM

**LSTM绕开了平凡RNN的梯度消失问题，是最为流行的RNN类网络之一**

## 实现RNN

**注意初始化参数U、V、W的时候不能采用全0初始化，因为会导致计算对称问题（不理解）。而是采用
随机初始化，一个推荐的的方法是在$[-\frac{1}{\sqrt n},\frac{1}{\sqrt n}]$区间内随机初始化**

## vanilla RNN的效果：
在教程提供的用于文本生成的RNN模型中，普通RNN不能产生有意义的文本，因为它不能学习到单词之间几步以外的依赖。这也是为什么RNN在开始问世时并没有流行起来的原因。


## Backpropagation Through Time and Vanishing Gradients(bptt和梯度消失)

### bptt
bptt只是bp的一个扩展，跟bp不同之处在于其中一些参数依赖前面的状态，而不仅仅是当前状态。这在参数求导时会利用链式法则，对t step前的每一步的导数加起来表示t步的参数的导数。

### 梯度消失问题
因为bptt对其中一些参数求导利用链式法则，当往前依赖越多，求导链越长，而问题就处在这里。tanh或sigmoid激活函数的导数的值在\[0,1\](sigmoid在\[0,0.25\])之间，在所有层之中，一些层的神经元激活函数值趋于0或1（饱和状态）,梯度到这里就趋于0,这将导致前面层的参数的梯度也趋于0，所以造成梯度消失。远离t(当前步)的层的梯度贡献为0，这解释了RNN不能处理长依赖。

**梯度消失问题不仅RNN有，这是所有深度前馈网络都会碰到的问题。只是RNN一般都很深（在语言模型中是句子的长度）,在RNN中这个问题特别明显。**

### 解决梯度消失
合理的初始化参数、正则、通过使用ReLU激活函数代替tanh或sigmoid函数、或者使用RNN的变体lSTM或GRU(LSTM的简化版)等等。


## LSTM
在RNN中，我们计算隐藏层状态$s_t=tanh(Ux_t+Ws_{t-1})$。LSTM单元做的正是同样的事情。**这是从宏观上理解LSTM的关键**

LSTM把原来的tanh结构改造成另一个结构。

**插图**

LSTM的关键是细胞状态(cell state)，通过称为“门”(gate)的结构来维持。LSTM有三个门,input门，forget门，output门，来保护和控制细胞状态。

这些都可以用几条等式表示。（待补充）

## GRU
只有两个“门”，一个reset门，一个update门。跟lstm比没有了内部存储器c，结构比lstm简单一点。
公式化表示（待补充）

## LSTM与GRU的比较
LSTM与GRU都可以解决梯度消失的问题（如何绕过？待理解），你可能会想知道：使用哪一个？ GRU是相当新的（2014年），其权衡还没有得到充分的探索。 根据门控递归神经网络的序贯建模实证评估和经常性网络结构实证研究的实证评估，并没有明确的赢家。 在许多任务中，两种体系结构都可以产生可比较的性能，而调整超参数（如层大小）可能比选择理想的体系结构更重要。 **GRU具有较少的参数（U和W较小），因此可能训练速度更快或需要更少的数据来推广。 另一方面，如果你有足够的数据，LSTM的更强大的表现力可能会带来更好的结果。**