---
title: 爬虫学习笔记
tags:
---


1. 对JavaScript渲染页面的网站，掌握起来有难度。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳。因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。

2. 代理的基本原理。爬虫过程可能出现“您的IP访问频率太高”这样的提示。出现这种现象的原因是网站采取了一些反爬虫措施。比如，服务器会检测某个IP在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封IP。解决这种情况的一种方法是使用代理。

3. urllib库的Request参数data必须传bytes（字节流）类型的。如果它是字典，可以先用urllib.parse模块里的urlencode()编码。例如：data = bytes(urllib.parse.urlencode({'word':'hello'}),encoding='utf-8')

4. urllib库有简单的用法，使用urlopen()、Request即可完成。也有高级的用法：使用Handler(包括代理、cookie等)和Opener(用handler来构建)。

5. 处理异常。网络不好的情况下，如果出现了异常，该怎么办呢？这时如果不处理这些异常，程序很可能因报错而终止运行，所以异常处理还是十分有必要的。

6. urllib的parse解析url。可以将url解析成几部分。几个方便有用的函数：urlencode()可以将字典类型参数序列化转为GET请求的参数，反序列化成字典类型用parse_qs()。quote()可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码！相应的unquote()可以复原。

7. 进阶的库使用：requests，它可以做到Urllib.request做的东西，并且更加容易和方便使用。实践中用这个库就可以了。

8. 正则表达式的使用。爬虫的核心就是爬取需要的内容，而正则表达式就可以帮我们在文本中匹配出需要的内容。

9. html解析库的使用。一长串复杂的正则表达式容易出错。幸好html也算是结构化的语言，使用XPath,Beautiful Soup, PyQuery等解析库使我们不用正则表达式就可以简单容易地获取我们想要地文本内容。实践中用pyquery库就行了（使用jquery语法，更方便熟悉jquery的人使用）。

10. ajax加载网页。我们知道，真实的数据其实都是一次次Ajax请求得到的，如果想要抓取这些数据，需要知道这些请求到底是怎么发送的，发往哪里，发了哪些参数。如果我们知道了这些，不就可以用Python模拟这个发送操作，获取到其中的结果了吗？

11. js动态加载的网页。对于这种网页，我们首先用逆向分析法，从网页的js代码入手找到自己需要的信息。实在不行可以用Selenium库+Chrome(PhantomJS更好，无显示浏览器) 模拟真实的浏览器环境，但是这样子速度不够快，所以不是很推荐。

12. 后面的就是进阶内容了，代理、验证码、框架等等。